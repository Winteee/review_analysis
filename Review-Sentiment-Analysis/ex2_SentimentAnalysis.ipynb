{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————语料预处理：数据导入————————\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (1,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewdata的数据格式为：\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 0 to 29999\n",
      "Data columns (total 4 columns):\n",
      "reviewbody    30000 non-null object\n",
      "score1        30000 non-null float64\n",
      "score2        30000 non-null float64\n",
      "score3        30000 non-null float64\n",
      "dtypes: float64(3), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "【over】\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #导入Pandas\n",
    "import numpy as np #导入Numpy\n",
    "import jieba #导入结巴分词\n",
    "import jieba.analyse\n",
    " \n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "print(\"————————语料预处理：数据导入————————\")\n",
    "reviewfile=pd.read_csv('review30000.csv') #读取训练语料完毕\n",
    "reviewdata = pd.DataFrame()\n",
    "reviewdata[\"reviewbody\"]= reviewfile[\"b.reviewbody\"]\n",
    "reviewdata[\"score1\"]= reviewfile[\"口味评分\"]\n",
    "reviewdata[\"score2\"]= reviewfile[\"环境评分\"]\n",
    "reviewdata[\"score3\"]= reviewfile[\"服务评分\"]\n",
    "reviewdata = reviewdata.dropna(axis=0,how='any') #去掉所有有缺失值的行\n",
    "print(\"reviewdata的数据格式为：\")\n",
    "print(reviewdata.info())\n",
    "print(\"【over】\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#【函数】查看pandas数据的信息\n",
    "def getinfo(inputdata):\n",
    "    print(\"文件的具体信息如下==========\")\n",
    "    info = inputdata.info()\n",
    "    print(info)\n",
    "    print(\"===========\")\n",
    "\n",
    "#【函数】分词&清洗无用词\n",
    "def cutwords(review_data):\n",
    "    cw = lambda x: list(jieba.cut(x)) \n",
    "    review_data['words'] = review_data[\"reviewbody\"].apply(cw)\n",
    "    print(\"——————————分词结束——————————\")\n",
    "    \n",
    "    delwordslist = \"，/,/。/的/！/～/、/（/）/ /./。/吧\".split(\"/\")\n",
    "    for i in review_data.index:\n",
    "        one = review_data.words[i]\n",
    "        for k in one:\n",
    "            if k in delwordslist:\n",
    "                one.remove(k)\n",
    "        i +=1\n",
    "    print(\"——————————清洗无用词结束——————————\")\n",
    "    return review_data\n",
    "\n",
    "#【函数】构建词频字典（输入的数据需要经过cutwords（））\n",
    "def dict_words(review_data):\n",
    "    w = []\n",
    "    for i in review_data[\"words\"]:\n",
    "        w.extend(i)\n",
    "    word_dict = pd.DataFrame(pd.Series(w).value_counts(),columns = [\"counts\"])\n",
    "    word_dict.to_csv(\"word_dict(词组映射).csv\",index=True,header=True)\n",
    "    word_dict['id'] = list(range(1, len(word_dict)+1))\n",
    "    getinfo(word_dict)\n",
    "    word_dict.to_csv(\"word_dict(词组映射).csv\",index=True,header=True)\n",
    "    print(\"词典构建完成，输出文件word_dict(词组映射).csv——————【over】\")\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "#【函数】词向量化\n",
    "def arraywords(review_data,word_dict):\n",
    "    maxlen = 50\n",
    "    review_data['sent'] = review_data['words'].apply(lambda x: list(word_dict['id'][x]))\n",
    "    #review_data['sent'] = list(sequence.pad_sequences(review_data['sent'], maxlen=maxlen))\n",
    "    review_data['sent'] = list(sequence.pad_sequences(review_data['sent'], maxlen=maxlen))\n",
    "    print(\"——————————词向量化结束——————————\")\n",
    "    return review_data\n",
    "\n",
    "#【函数】给数据打标:\n",
    "def markwords(inputdata):\n",
    "    inputdata[\"totalscore\"] = inputdata.score1+inputdata.score2+inputdata.score3 \n",
    "    inputdata[\"pos_or_neg\"] = pd.Series()\n",
    "    pos = inputdata[inputdata.totalscore>11]\n",
    "    pos[\"pos_or_neg\"]=1\n",
    "    neg = inputdata[inputdata.totalscore<4]\n",
    "    neg[\"pos_or_neg\"] = 0\n",
    "    inputdata = pos.append(neg)\n",
    "    print(\"——————————标注结束——————————\")\n",
    "    print(\"得到{}条正向评论，{}条负向评论\".format(len(pos),len(neg)))\n",
    "    return inputdata\n",
    "\n",
    "#【函数】训练过程可视化图表\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#写一个LossHistory类，保存loss和acc\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/t9/186m3wkn1c1c1y1m9dmv76zh0000gn/T/jieba.cache\n",
      "Loading model cost 1.078 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————分词结束——————————\n",
      "——————————清洗无用词结束——————————\n",
      "文件的具体信息如下==========\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 46955 entries, 了 to 炸龙利\n",
      "Data columns (total 2 columns):\n",
      "counts    46955 non-null int64\n",
      "id        46955 non-null int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "===========\n",
      "词典构建完成，输出文件word_dict(词组映射).csv——————【over】\n"
     ]
    }
   ],
   "source": [
    "a = cutwords(reviewdata)\n",
    "word_dict = dict_words(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————词向量化结束——————————\n",
      "完成（词向量化），得到 reviewdata_sented ：\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 0 to 29999\n",
      "Data columns (total 8 columns):\n",
      "reviewbody    30000 non-null object\n",
      "score1        30000 non-null float64\n",
      "score2        30000 non-null float64\n",
      "score3        30000 non-null float64\n",
      "words         30000 non-null object\n",
      "sent          30000 non-null object\n",
      "totalscore    30000 non-null float64\n",
      "pos_or_neg    0 non-null float64\n",
      "dtypes: float64(5), object(3)\n",
      "memory usage: 3.3+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————————标注结束——————————\n",
      "得到13912条正向评论，1185条负向评论\n",
      "完成（数据情感极性标注），输出文件reviewdata_markeded(词组映射).csv——————【over】\n"
     ]
    }
   ],
   "source": [
    "#开始调用函数处理数据：\n",
    "\n",
    "reviewdata_sented = arraywords(a,word_dict)\n",
    "print(\"完成（词向量化），得到 reviewdata_sented ：\")\n",
    "print(reviewdata_sented.info())\n",
    "reviewdata_marked = markwords(reviewdata_sented)\n",
    "reviewdata_marked.to_csv(\"reviewdata_marked(词组映射).csv\",index=True,header=True)\n",
    "print(\"完成（数据情感极性标注），输出文件reviewdata_markeded(词组映射).csv——————【over】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建的模型信息为：\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 256)         12020736  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 12,546,562\n",
      "Trainable params: 12,546,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "下面开始训练\n",
      "Train on 11322 samples, validate on 3775 samples\n",
      "Epoch 1/10\n",
      "11322/11322 [==============================] - 201s 18ms/step - loss: 0.1537 - acc: 0.9461 - val_loss: 0.0954 - val_acc: 0.9669\n",
      "Epoch 2/10\n",
      "11322/11322 [==============================] - 216s 19ms/step - loss: 0.0561 - acc: 0.9832 - val_loss: 0.1233 - val_acc: 0.9574\n",
      "Epoch 3/10\n",
      "11322/11322 [==============================] - 211s 19ms/step - loss: 0.0352 - acc: 0.9911 - val_loss: 0.1383 - val_acc: 0.9608\n",
      "Epoch 4/10\n",
      "11322/11322 [==============================] - 222s 20ms/step - loss: 0.0204 - acc: 0.9951 - val_loss: 0.1366 - val_acc: 0.9616\n",
      "Epoch 5/10\n",
      "11322/11322 [==============================] - 201s 18ms/step - loss: 0.0163 - acc: 0.9953 - val_loss: 0.1531 - val_acc: 0.9574\n",
      "Epoch 6/10\n",
      "11322/11322 [==============================] - 208s 18ms/step - loss: 0.0107 - acc: 0.9974 - val_loss: 0.2298 - val_acc: 0.9579\n",
      "Epoch 7/10\n",
      "11322/11322 [==============================] - 205s 18ms/step - loss: 0.0107 - acc: 0.9970 - val_loss: 0.1727 - val_acc: 0.9542\n",
      "Epoch 8/10\n",
      " 8032/11322 [====================>.........] - ETA: 1:03 - loss: 0.0053 - acc: 0.9984"
     ]
    }
   ],
   "source": [
    "#进行神经网络模型搭建\n",
    "from __future__ import absolute_import \n",
    "from __future__ import print_function\n",
    "\n",
    "#from keras.utils.visualize_util import plot\n",
    "from sklearn import model_selection\n",
    "from keras.utils import np_utils\n",
    "\n",
    "seed = 7\n",
    "X = np.array(list(reviewdata_marked['sent']))\n",
    "Y = np.array(list(reviewdata_marked['pos_or_neg']))\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.25, random_state=seed)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_dict)+1, 256))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#plot(model, to_file='model1.png',show_shapes=True)\n",
    "\n",
    "print(\"构建的模型信息为：\")\n",
    "model.summary()\n",
    "model.get_config()\n",
    "\n",
    "print(\"下面开始训练\")\n",
    "\n",
    "\n",
    "history = LossHistory()\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10,validation_data=(x_test, y_test),callbacks=[history])#128时，每个epoch训练92s\n",
    "history.loss_plot('epoch')\n",
    "print(\"【over】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3775/3775 [==============================] - 11s 3ms/step\n",
      "Test accuracy = : [0.22740127703144111, 0.9592052977764054]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=1) \n",
    "#【evaluate函数返回什么？？？】\n",
    "#Scalar test loss (if the model has no metrics)or list of scalars (if the model computes other metrics).\n",
    "#The attribute `model.metrics_names` will give you\n",
    "#the display labels for the scalar outputs.\n",
    "print (\"Test accuracy = :\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new(reviewdata):\n",
    "    reviewdata_sent = arraywords(reviewdata,word_dict)\n",
    "    x = np.array(list(reviewdata_sent['sent']))\n",
    "    y = model.predict_classes(x)\n",
    "    y_score = model.predict(x)\n",
    "    y_out = y.tolist()\n",
    "    y_score_out = y_score.tolist()\n",
    "    predict_result = pd.DataFrame()\n",
    "    predict_result[\"reviewinput\"] = pd.Series(reviewdata.reviewbody)\n",
    "    predict_result[\"p:pos_or_neg\"] = y_out\n",
    "    predict_result[\"p:score\"] = y_score_out\n",
    "    return predict_result\n",
    "\n",
    "#输入一个评论来预测\n",
    "def predict_one(review):\n",
    "    words = re.findall('[\\x80-\\xff]{3}|[\\w\\W]', review)\n",
    "    print(\"words = \",words)\n",
    "    sent = []\n",
    "    for x in words:\n",
    "        get_id = list(word_dict[\"id\"][word_dict.index == x].values)\n",
    "        sent.extend(get_id)\n",
    "    print(\"sent = \",sent)\n",
    "    x_input = sequence.pad_sequences([sent], maxlen=25)\n",
    "    print(\"x_input = \",x_input)\n",
    "    predicted = model.predict_classes(x_input, verbose=0)\n",
    "    return predicted\n",
    "\n",
    "def new_data_predict(new_comment):\n",
    "    words = list(jieba.cut(new_comment))\n",
    "    sent = [word_dict['id'][x] for x in words]\n",
    "    xn = sequence.pad_sequences([sent], maxlen=50)\n",
    "    yn = model.predict_classes(xn,verbose = 0)\n",
    "    y_score = model.predict(xn,verbose = 0)\n",
    "    print(\"预测结果：\",yn,y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果： [1] [[ 0.00379209  0.99620795]]\n"
     ]
    }
   ],
   "source": [
    "a = \"我就呵呵了，前面的评论是怎么得出这里好吃的结论的？菜都烧烂了\"\n",
    "new_data_predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
